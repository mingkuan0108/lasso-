import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from numpy.linalg import norm, cholesky, solve
import warnings
from tqdm import tqdm
import pandas as pd
from time import time
import os

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# åˆ›å»ºç»“æœä¿å­˜ç›®å½•
os.makedirs('experiment_results', exist_ok=True)

# ====================================================================
# 1. å‚æ•°è®¾ç½®
# ====================================================================
# æ•°æ®ç»´åº¦ç»„åˆ
n_p_pairs = [
    (100, 20),  # å°æ ·æœ¬+ä½ç»´
    (200, 50),  # ä¸­æ ·æœ¬+ä¸­ç»´
    (500, 100),  # å¤§æ ·æœ¬+ä¸­ç»´
    (200, 200)  # ä¸­æ ·æœ¬+é«˜ç»´ï¼ˆp=nï¼‰
]

# å®éªŒå‚æ•°
max_iter = 100
n_trials = 30
lambda_ratio = 0.1

# ç®—æ³•é…ç½®ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰
algo_configs = {
    # --- [1] åæ ‡ä¸‹é™ç±» ---
    'BCD (Adaptive)': {'color': 'firebrick', 'style': '-', 'width': 2.5, 'group': 'Coordinate Descent'},

    # --- [2] Huberå¹³æ»‘ç±» ---
    'Huber Gradient (Adaptive)': {'color': 'limegreen', 'style': '-', 'width': 2.5, 'group': 'Huber'},
    'Huber Gradient (Accel+Adaptive)': {'color': 'forestgreen', 'style': '-', 'width': 2.5, 'group': 'Huber'},
    'Huber (Accel+Restart+Adaptive)': {'color': 'darkgreen', 'style': '--', 'width': 2.5, 'group': 'Huber'},

    # --- [3] è¿‘ç«¯æ¢¯åº¦ç±» ---
    'FISTA (Adaptive)': {'color': 'darkblue', 'style': '-', 'width': 2.5, 'group': 'Proximal Gradient'},
    'FISTA (Restart+Adaptive)': {'color': 'blue', 'style': '--', 'width': 2.5, 'group': 'Proximal Gradient'},

    # --- [4] åˆ†è£‚ä¹˜å­ç±» ---
    'ADMM (rho=0.5)': {'color': 'orange', 'style': '-', 'width': 2, 'group': 'ADMM'},
    'ADMM (rho=1.0)': {'color': 'red', 'style': '--', 'width': 2, 'group': 'ADMM'},  # æ”¹ä¸ºçº¢è‰²è™šçº¿
    'ADMM (rho=2.0)': {'color': 'purple', 'style': ':', 'width': 2, 'group': 'ADMM'},  # æ”¹ä¸ºç´«è‰²ç‚¹çº¿

    # --- [5] éšæœºä¼˜åŒ–ç±» ---
    'SGD (Adaptive)': {'color': 'brown', 'style': '-', 'width': 2, 'group': 'Stochastic'},

    # --- [6] æ¬¡æ¢¯åº¦ç±» ---
    'Subgradient (Adaptive)': {'color': 'gray', 'style': '-', 'width': 2, 'group': 'Subgradient'}
}

# æŒ‰ç®—æ³•åˆ†ç»„
algorithm_groups = {
    'Coordinate Descent': ['BCD (Adaptive)'],
    'Proximal Gradient': ['FISTA (Adaptive)', 'FISTA (Restart+Adaptive)'],
    'ADMM': ['ADMM (rho=0.5)', 'ADMM (rho=1)', 'ADMM (rho=2)'],
    'Stochastic': ['SGD (Adaptive)'],
    'Subgradient': ['Subgradient (Adaptive)']
}

# åˆå§‹åŒ–ç»“æœå­˜å‚¨
all_results = {
    (n, p): {name: [] for name in algo_configs.keys()}
    for n, p in n_p_pairs
}
all_trial_times = {
    (n, p): {name: [] for name in algo_configs.keys()}
    for n, p in n_p_pairs
}

# ====================================================================
# 2. è¾…åŠ©å‡½æ•°
# ====================================================================
def soft_threshold(x, tau):
    """è½¯é˜ˆå€¼å‡½æ•°"""
    return np.sign(x) * np.maximum(np.abs(x) - tau, 0)

def lasso_objective(beta, X, y, n, lam):
    """è®¡ç®—LASSOç›®æ ‡å‡½æ•°å€¼"""
    residual = X @ beta - y
    l2_loss = (0.5 / n) * (residual @ residual)
    l1_norm = lam * norm(beta, 1)
    return l2_loss + l1_norm

def get_algo_params(algo_name, n, p):
    """æ ¹æ®ç®—æ³•åç§°å’Œæ•°æ®ç»´åº¦è·å–è‡ªé€‚åº”å‚æ•°"""
    params = {}
    # ç»´åº¦ç‰¹å¾åˆ¤æ–­
    is_high_dim = p >= 100
    is_large_sample = n >= 500
    is_square = n == p

    # BCDå‚æ•°
    if 'BCD' in algo_name:
        params.update({
            'block_size': min(20, p // 5) if is_high_dim else 1,
            'max_iter_adjust': int(max_iter * 0.8 if is_high_dim else max_iter)
        })
    # FISTAå‚æ•°
    elif 'FISTA' in algo_name:
        params.update({
            'L_scale': 1.2 if is_high_dim else 1.0,
            'alpha_scale': 0.8 if is_large_sample else 1.0,
            'restart_threshold': 0.1 if is_high_dim else 0.0
        })
    # ADMMå‚æ•°
    elif 'ADMM' in algo_name:
        params.update({
            'reuse_cholesky': True if is_large_sample else False
        })
    # SGDå‚æ•°
    elif 'SGD' in algo_name:
        params.update({
            'batch_size': min(64, n // 10) if is_large_sample else 32,
            'lr': 0.005 if is_high_dim else 0.01,
            'lr_decay': 0.99 if is_large_sample else 1.0
        })
    # Subgradientå‚æ•°
    elif 'Subgradient' in algo_name:
        params.update({
            'lr': 0.003 if is_high_dim else 0.01,
            'lr_decay': 0.98 if is_large_sample else 1.0
        })
    return params

# ====================================================================
# 3. ç®—æ³•æ±‚è§£å™¨ï¼ˆåˆ é™¤Huberç›¸å…³å‡½æ•°ï¼‰
# ====================================================================
# ==========================================
# 3.1 BCDï¼ˆåˆ†å—åæ ‡ä¸‹é™ï¼‰
# ==========================================
def bcd_adaptive(X, y, n, p, lam, max_iter, f_star, params):
    """è‡ªé€‚åº”å—åæ ‡ä¸‹é™"""
    beta = np.zeros(p)
    history = []
    block_size = params['block_size']
    max_iter_adj = params['max_iter_adjust']

    # åˆ†å—ç´¢å¼•
    blocks = [np.arange(i, min(i + block_size, p)) for i in range(0, p, block_size)]

    # é¢„è®¡ç®—
    A_j = np.zeros(p)
    for j in range(p):
        A_j[j] = (X[:, j] @ X[:, j]) / n
        if A_j[j] == 0:
            A_j[j] = 1e-8

    for k in range(max_iter_adj):
        # éšæœºåŒ–å—é¡ºåºä»¥æå‡æ”¶æ•›
        np.random.shuffle(blocks)
        for block in blocks:
            for j in block:
                old_beta_j = beta[j]
                residual_no_j = y - (X @ beta - X[:, j] * old_beta_j)
                c_j = (X[:, j] @ residual_no_j) / n
                beta[j] = soft_threshold(c_j / A_j[j], lam / A_j[j])

        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))

    # è¡¥å…¨é•¿åº¦
    if len(history) < max_iter:
        history += [history[-1]] * (max_iter - len(history))
    return history

# ==========================================
# 3.2 FISTAç±»
# ==========================================
def fista_adaptive(X, y, n, p, lam, max_iter, f_star, params):
    """è‡ªé€‚åº”FISTA"""
    beta = np.zeros(p)
    z = np.zeros(p)
    t = 1.0
    history = []

    L = norm(X.T @ X / n, ord=2) * params['L_scale']
    alpha = (1.0 / L) * params['alpha_scale'] if L > 0 else 0.01

    for k in range(max_iter):
        beta_old = beta.copy()
        # è¿‘ç«¯æ¢¯åº¦æ­¥éª¤
        grad_z = (X.T @ (X @ z - y)) / n
        beta = soft_threshold(z - alpha * grad_z, alpha * lam)
        # NesterovåŠ é€Ÿ
        t_new = (1 + np.sqrt(1 + 4 * t ** 2)) / 2
        z = beta + ((t - 1) / t_new) * (beta - beta_old)
        t = t_new

        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))
    return history

def fista_restart_adaptive(X, y, n, p, lam, max_iter, f_star, params):
    """è‡ªé€‚åº”é‡å¯FISTA"""
    beta = np.zeros(p)
    z = np.zeros(p)
    t = 1.0
    history = []

    L = norm(X.T @ X / n, ord=2) * params['L_scale']
    alpha = (1.0 / L) * params['alpha_scale'] if L > 0 else 0.01
    restart_threshold = params['restart_threshold']

    for k in range(max_iter):
        beta_old = beta.copy()
        # è¿‘ç«¯æ¢¯åº¦æ­¥éª¤
        grad_z = (X.T @ (X @ z - y)) / n
        beta_new = soft_threshold(z - alpha * grad_z, alpha * lam)
        # åŠ¨æ€é‡å¯é€»è¾‘
        if np.dot(z - beta_new, beta_new - beta_old) > restart_threshold:
            t_new = 1.0
            z = beta_new
        else:
            t_new = (1 + np.sqrt(1 + 4 * t ** 2)) / 2
            z = beta_new + ((t - 1) / t_new) * (beta_new - beta_old)

        beta = beta_new
        t = t_new
        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))
    return history

# ==========================================
# 3.3 ADMM
# ==========================================
def admm_adaptive(X, y, n, p, lam, rho, max_iter, f_star, params):
    """è‡ªé€‚åº”ADMM"""
    beta = np.zeros(p)
    z = np.zeros(p)
    u = np.zeros(p)
    history = []
    I = np.identity(p)

    # å¤§æ ·æœ¬å¤ç”¨Choleskyåˆ†è§£
    if params['reuse_cholesky']:
        L_cho = cholesky(X.T @ X / n + rho * I)

    for k in range(max_iter):
        # x-å­é—®é¢˜
        rhs = (X.T @ y / n) + rho * (z - u)
        if params['reuse_cholesky']:
            beta = solve(L_cho.T, solve(L_cho, rhs))
        else:
            beta = np.linalg.solve(X.T @ X / n + rho * I, rhs)
        # z-å­é—®é¢˜ï¼ˆè½¯é˜ˆå€¼ï¼‰
        z = soft_threshold(beta + u, lam / rho)
        # å¯¹å¶å˜é‡æ›´æ–°
        u = u + beta - z

        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))
    return history

# ==========================================
# 3.4 éšæœºæ¢¯åº¦ä¸‹é™
# ==========================================
def sgd_adaptive(X, y, n, p, lam, max_iter, f_star, params):
    """è‡ªé€‚åº”éšæœºæ¢¯åº¦ä¸‹é™"""
    beta = np.zeros(p)
    history = []
    batch_size = params['batch_size']
    lr = params['lr']
    lr_decay = params['lr_decay']

    for k in range(max_iter):
        # åŠ¨æ€æ‰¹æ¬¡é‡‡æ ·
        idx = np.random.choice(n, size=batch_size, replace=False)
        X_batch = X[idx]
        y_batch = y[idx]
        # æ¢¯åº¦è®¡ç®—
        grad = (X_batch.T @ (X_batch @ beta - y_batch)) / batch_size
        # æ¢¯åº¦æ›´æ–°
        beta = beta - lr * grad
        beta = soft_threshold(beta, lr * lam)
        # å­¦ä¹ ç‡è¡°å‡
        lr *= lr_decay

        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))
    return history

# ==========================================
# 3.5 æ¬¡æ¢¯åº¦ä¸‹é™
# ==========================================
def subgradient_adaptive(X, y, n, p, lam, max_iter, f_star, params):
    """è‡ªé€‚åº”æ¬¡æ¢¯åº¦ä¸‹é™"""
    beta = np.zeros(p)
    history = []
    lr = params['lr']
    lr_decay = params['lr_decay']

    for k in range(max_iter):
        # æ¬¡æ¢¯åº¦è®¡ç®—
        grad = (X.T @ (X @ beta - y)) / n
        beta = beta - lr * (grad + lam * np.sign(beta))
        # å­¦ä¹ ç‡è¡°å‡
        lr *= lr_decay

        subopt = lasso_objective(beta, X, y, n, lam) - f_star
        history.append(max(subopt, 1e-15))
    return history

# ====================================================================
# 4. å®éªŒä¸»å¾ªç¯
# ====================================================================
print(f"Starting {n_trials} trials for {len(n_p_pairs)} (n,p) combinations...")
print("=" * 60)

for idx, (n, p) in enumerate(n_p_pairs):
    print(f"\n=== Processing (n={n}, p={p}) [{idx + 1}/{len(n_p_pairs)}] ===")
    # åˆ›å»ºå½“å‰ç»´åº¦çš„è¿›åº¦æ¡
    for i in tqdm(range(n_trials), desc=f"Trials for (n={n},p={p})"):
        # æ•°æ®ç”Ÿæˆ
        X = np.random.randn(n, p)
        true_beta = np.zeros(p)
        n_informative = min(10, p // 2)
        true_beta[:n_informative] = np.random.uniform(-5, 5, n_informative)
        y = X @ true_beta + np.random.randn(n) * 0.5

        # æ­£åˆ™åŒ–å‚æ•°è®¡ç®—
        lam_max = norm(X.T @ y, ord=np.inf) / n
        lam = lam_max * lambda_ratio

        # è®¡ç®—æœ€ä¼˜è§£ï¼ˆä½¿ç”¨scikit-learnï¼‰
        lasso_sklearn = Lasso(alpha=lam, fit_intercept=False, tol=1e-14, max_iter=20000)
        lasso_sklearn.fit(X, y)
        f_star = lasso_objective(lasso_sklearn.coef_, X, y, n, lam)

        # =================== è¿è¡Œæ‰€æœ‰ç®—æ³• ===================
        # 1. BCDç®—æ³•
        if 'BCD (Adaptive)' in algo_configs:
            params = get_algo_params('BCD (Adaptive)', n, p)
            start = time()
            all_results[(n, p)]['BCD (Adaptive)'].append(
                bcd_adaptive(X, y, n, p, lam, max_iter, f_star, params)
            )
            all_trial_times[(n, p)]['BCD (Adaptive)'].append(time() - start)

        # 2. FISTAç®—æ³•ç±»
        if 'FISTA (Adaptive)' in algo_configs:
            params = get_algo_params('FISTA (Adaptive)', n, p)
            start = time()
            all_results[(n, p)]['FISTA (Adaptive)'].append(
                fista_adaptive(X, y, n, p, lam, max_iter, f_star, params)
            )
            all_trial_times[(n, p)]['FISTA (Adaptive)'].append(time() - start)
        if 'FISTA (Restart+Adaptive)' in algo_configs:
            params = get_algo_params('FISTA (Restart+Adaptive)', n, p)
            start = time()
            all_results[(n, p)]['FISTA (Restart+Adaptive)'].append(
                fista_restart_adaptive(X, y, n, p, lam, max_iter, f_star, params)
            )
            all_trial_times[(n, p)]['FISTA (Restart+Adaptive)'].append(time() - start)

        # 3. ADMMç®—æ³•ç±»
        for rho in [0.5, 1.0, 2.0]:
            algo_name = f'ADMM (rho={rho})'
            if algo_name in algo_configs:
                params = get_algo_params(algo_name, n, p)
                start = time()
                all_results[(n, p)][algo_name].append(
                    admm_adaptive(X, y, n, p, lam, rho, max_iter, f_star, params)
                )
                all_trial_times[(n, p)][algo_name].append(time() - start)

        # 4. SGDç®—æ³•
        if 'SGD (Adaptive)' in algo_configs:
            params = get_algo_params('SGD (Adaptive)', n, p)
            start = time()
            all_results[(n, p)]['SGD (Adaptive)'].append(
                sgd_adaptive(X, y, n, p, lam, max_iter, f_star, params)
            )
            all_trial_times[(n, p)]['SGD (Adaptive)'].append(time() - start)

        # 5. Subgradientç®—æ³•
        if 'Subgradient (Adaptive)' in algo_configs:
            params = get_algo_params('Subgradient (Adaptive)', n, p)
            start = time()
            all_results[(n, p)]['Subgradient (Adaptive)'].append(
                subgradient_adaptive(X, y, n, p, lam, max_iter, f_star, params)
            )
            all_trial_times[(n, p)]['Subgradient (Adaptive)'].append(time() - start)

    # æ˜¾ç¤ºå½“å‰ç»´åº¦å®Œæˆè¿›åº¦
    print(f"âœ“ Completed (n={n}, p={p})")

# ====================================================================
# 5. å¯è§†åŒ–åˆ†æ
# ====================================================================
print("\n" + "=" * 60)
print("All trials complete. Generating visualizations...")
print("=" * 60)

# 5.1 æ”¶æ•›æ›²çº¿å¯¹æ¯”ï¼ˆå­å›¾å¸ƒå±€ï¼‰
fig, axes = plt.subplots(2, 2, figsize=(18, 14))
axes = axes.flatten()
k_axis = np.arange(1, max_iter + 1)

for idx, (n, p) in enumerate(n_p_pairs):
    ax = axes[idx]
    results = all_results[(n, p)]

    # ä¸ºæ¯ä¸ªç®—æ³•ç»˜åˆ¶æ›²çº¿
    for algo_name in algo_configs.keys():
        if algo_name not in results or len(results[algo_name]) == 0:
            continue

        histories = results[algo_name]
        data_matrix = np.array(histories)
        min_len = min([len(h) for h in histories])
        data_matrix = data_matrix[:, :min_len]
        current_k_axis = k_axis[:min_len]
        mean_curve = np.mean(data_matrix, axis=0)

        cfg = algo_configs[algo_name]
        color = cfg['color']
        style = cfg['style']
        width = cfg.get('width', 2)

        # ç»˜åˆ¶å‡å€¼æ›²çº¿
        ax.plot(current_k_axis, mean_curve, color=color, linestyle=style,
                linewidth=width, label=algo_name)

    # å­å›¾è®¾ç½®
    ax.set_yscale('log')
    ax.set_xlabel('Iteration k', fontsize=12)
    ax.set_ylabel('Suboptimality $f(x_k) - f^*$', fontsize=12)
    ax.set_title(f'Convergence: (n={n}, p={p})', fontsize=14, fontweight='bold')
    ax.grid(True, which="both", ls="--", alpha=0.4)
    ax.set_ylim(bottom=1e-12)
    ax.set_xlim(0, max_iter)

# åˆ›å»ºç»Ÿä¸€çš„å›¾ä¾‹ï¼ˆæ”¾åœ¨å›¾è¡¨å¤–é¢ï¼‰
# æ”¶é›†æ‰€æœ‰ç®—æ³•çš„å¥æŸ„å’Œæ ‡ç­¾
handles, labels = [], []
for algo_name in algo_configs.keys():
    # ä¸ºæ¯ä¸ªç®—æ³•åˆ›å»ºä¸€ä¸ªä»£ç†çº¿æ¡ç”¨äºå›¾ä¾‹
    cfg = algo_configs[algo_name]
    color = cfg['color']
    style = cfg['style']
    width = cfg.get('width', 2)

    # åˆ›å»ºä»£ç†çº¿æ¡
    proxy_line = plt.Line2D([0], [0], color=color, linestyle=style,
                            linewidth=width, label=algo_name)
    handles.append(proxy_line)
    labels.append(algo_name)

# å°†å›¾ä¾‹æ”¾åœ¨å›¾è¡¨ä¸‹æ–¹
fig.legend(handles, labels, loc='lower center', ncol=3, fontsize=9,
           framealpha=0.95, title='Algorithms (Adaptive)', title_fontsize=10)

# è°ƒæ•´å¸ƒå±€ï¼Œä¸ºå›¾ä¾‹ç•™å‡ºç©ºé—´
plt.tight_layout(rect=[0, 0.05, 1, 0.95])

plt.suptitle('LASSO Optimization: Adaptive Algorithms Comparison', fontsize=16, fontweight='bold', y=0.98)
plt.savefig('experiment_results/convergence_adaptive.png', dpi=300, bbox_inches='tight')
plt.show()

# 5.2 è¿è¡Œæ—¶é—´å¯¹æ¯”ï¼ˆåˆ†ç»„æŸ±çŠ¶å›¾ï¼‰
fig, ax = plt.subplots(1, 1, figsize=(14, 8))
# å‡†å¤‡æ•°æ®
algorithms = list(algo_configs.keys())
x = np.arange(len(algorithms))
width = 0.15  # æ¯ä¸ªæŸ±å­çš„å®½åº¦
colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # ä¸åŒ(n,p)ç»„åˆçš„é¢œè‰²

# ç»˜åˆ¶æŸ±çŠ¶å›¾
for idx, (n, p) in enumerate(n_p_pairs):
    avg_times = []
    for algo in algorithms:
        if algo in all_trial_times[(n, p)] and len(all_trial_times[(n, p)][algo]) > 0:
            avg_times.append(np.mean(all_trial_times[(n, p)][algo]))
        else:
            avg_times.append(0)
    # è®¡ç®—ä½ç½®åç§»
    offset = (idx - len(n_p_pairs) / 2 + 0.5) * width
    ax.bar(x + offset, avg_times, width, label=f'(n={n}, p={p})', color=colors_bar[idx], edgecolor='black')

# å›¾è¡¨è®¾ç½®
ax.set_xlabel('Adaptive Algorithms', fontsize=12)
ax.set_ylabel('Average Runtime (s)', fontsize=12)
ax.set_title('Runtime Comparison: Adaptive Algorithms by (n,p) Combinations', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(algorithms, rotation=45, ha='right', fontsize=10)
ax.legend(loc='upper left', fontsize=10)
ax.grid(True, axis='y', ls='--', alpha=0.4)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for idx, (n, p) in enumerate(n_p_pairs):
    avg_times = []
    for algo in algorithms:
        if algo in all_trial_times[(n, p)] and len(all_trial_times[(n, p)][algo]) > 0:
            avg_times.append(np.mean(all_trial_times[(n, p)][algo]))
        else:
            avg_times.append(0)
    offset = (idx - len(n_p_pairs) / 2 + 0.5) * width
    for j, time_val in enumerate(avg_times):
        if time_val > 0:  # åªæ˜¾ç¤ºéé›¶å€¼
            ax.text(j + offset, time_val + 0.001, f'{time_val:.3f}',
                    ha='center', va='bottom', fontsize=8, rotation=90)

plt.tight_layout()
plt.savefig('experiment_results/runtime_adaptive.png', dpi=300, bbox_inches='tight')
plt.show()

# ====================================================================
# 6. æ•°æ®åˆ†æä¸æŠ¥å‘Š
# ====================================================================
print("\n" + "=" * 60)
print("Generating comprehensive analysis report...")
print("=" * 60)

def generate_comprehensive_report():
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    report_data = []
    for (n, p) in n_p_pairs:
        trial_times = all_trial_times[(n, p)]
        results = all_results[(n, p)]
        for algo_name in algo_configs.keys():
            if algo_name not in trial_times or len(trial_times[algo_name]) == 0:
                continue
            # åŸºç¡€æŒ‡æ ‡
            avg_time = np.mean(trial_times[algo_name])
            std_time = np.std(trial_times[algo_name])
            # æ”¶æ•›æ€§èƒ½æŒ‡æ ‡
            if algo_name in results and len(results[algo_name]) > 0:
                histories = results[algo_name]
                final_subopts = [h[-1] for h in histories if len(h) > 0]
                avg_final_subopt = np.mean(final_subopts) if final_subopts else np.inf
                std_final_subopt = np.std(final_subopts) if len(final_subopts) > 1 else 0
                # æ”¶æ•›è¿­ä»£æ•°ï¼ˆè¾¾åˆ°1e-3ï¼‰
                conv_iters = []
                for h in histories:
                    for iter_idx, val in enumerate(h):
                        if val < 1e-3:
                            conv_iters.append(iter_idx + 1)
                            break
                    else:
                        conv_iters.append(np.inf)
                avg_conv_iter = np.mean(conv_iters) if conv_iters else np.inf
                success_rate = np.sum(np.array(conv_iters) < np.inf) / len(conv_iters) * 100
            else:
                avg_final_subopt = np.inf
                std_final_subopt = np.inf
                avg_conv_iter = np.inf
                success_rate = 0
            # æ¯è¿­ä»£è€—æ—¶
            iter_time = avg_time / max_iter if max_iter > 0 else np.inf
            # ç®—æ³•ç»„ä¿¡æ¯
            algo_group = algo_configs[algo_name]['group']
            report_data.append({
                'n': n,
                'p': p,
                'Algorithm': algo_name,
                'Algorithm Group': algo_group,
                'Avg Runtime (s)': round(avg_time, 4),
                'Std Runtime (s)': round(std_time, 4),
                'Avg Final Suboptimality': f"{avg_final_subopt:.2e}",
                'Std Final Suboptimality': f"{std_final_subopt:.2e}" if std_final_subopt < np.inf else 'Inf',
                'Avg Iter to 1e-3': round(avg_conv_iter, 1) if not np.isinf(avg_conv_iter) else 'Inf',
                'Success Rate (%)': round(success_rate, 1),
                'Time per Iter (ms)': round(iter_time * 1000, 2),
                'Raw Final Suboptimality': avg_final_subopt
            })
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    df = pd.DataFrame(report_data)
    df.to_csv('experiment_results/comprehensive_analysis_adaptive.csv', index=False, encoding='utf-8-sig')
    # æ‰“å°å…³é”®æ±‡æ€»
    print("\n=== è‡ªé€‚åº”ç®—æ³•å®éªŒå…³é”®ç»“æœæ±‡æ€» ===")
    print("=" * 80)
    # æŒ‰ç®—æ³•ç»„æ±‡æ€»
    print("\n1. æŒ‰ç®—æ³•ç»„å¹³å‡æ€§èƒ½:")
    print("-" * 80)
    group_summary = df.groupby('Algorithm Group').agg({
        'Avg Runtime (s)': 'mean',
        'Raw Final Suboptimality': 'mean',
        'Success Rate (%)': 'mean'
    }).round(4)
    group_summary['Avg Final Suboptimality'] = group_summary['Raw Final Suboptimality'].apply(
        lambda x: f"{x:.2e}"
    )
    group_summary = group_summary.drop('Raw Final Suboptimality', axis=1)
    group_summary = group_summary.sort_values('Avg Runtime (s)')
    print(group_summary.to_string())
    # æŒ‰ç»´åº¦æ±‡æ€»
    print("\n2. æŒ‰æ•°æ®ç»´åº¦æœ€ä¼˜ç®—æ³•:")
    print("-" * 80)
    for (n, p) in n_p_pairs:
        dim_data = df[(df['n'] == n) & (df['p'] == p)]
        if len(dim_data) > 0:
            # æœ€å¿«ç®—æ³•
            fastest = dim_data.loc[dim_data['Avg Runtime (s)'].idxmin()]
            # æœ€ç²¾ç¡®ç®—æ³•ï¼ˆæ’é™¤å¤±è´¥æƒ…å†µï¼‰
            valid_data = dim_data[dim_data['Success Rate (%)'] > 0]
            if len(valid_data) > 0:
                most_accurate = valid_data.loc[valid_data['Raw Final Suboptimality'].idxmin()]
            else:
                most_accurate = fastest
            print(f"(n={n}, p={p}):")
            print(f"  æœ€å¿«ç®—æ³•: {fastest['Algorithm']} ({fastest['Avg Runtime (s)']:.3f}s)")
            print(f"  æœ€ç²¾ç¡®ç®—æ³•: {most_accurate['Algorithm']} (æ¬¡ä¼˜æ€§: {most_accurate['Avg Final Suboptimality']})")
            print(
                f"  æ”¶æ•›æˆåŠŸç‡æœ€é«˜: {dim_data.loc[dim_data['Success Rate (%)'].idxmax()]['Algorithm']} ({dim_data['Success Rate (%)'].max():.1f}%)")
            print()
    # ç®—æ³•æ¨è
    print("\n3. ç®—æ³•é€‰æ‹©æ¨è:")
    print("-" * 80)
    recommendations = {
        "(100, 20) - å°æ ·æœ¬ä½ç»´": "BCD (Adaptive) æˆ– FISTA (Adaptive)",
        "(200, 50) - ä¸­æ ·æœ¬ä¸­ç»´": "FISTA (Restart+Adaptive) æˆ– ADMM (rho=1)",
        "(500, 100) - å¤§æ ·æœ¬ä¸­ç»´": "SGD (Adaptive) æˆ– ADMM (rho=1)",
        "(200, 200) - ä¸­æ ·æœ¬é«˜ç»´": "FISTA (Restart+Adaptive) æˆ– ADMM (rho=0.5)"
    }
    for scenario, recommendation in recommendations.items():
        print(f"  {scenario}: {recommendation}")
    print("\n" + "=" * 80)
    print("è¯¦ç»†æ•°æ®å·²ä¿å­˜è‡³: experiment_results/comprehensive_analysis_adaptive.csv")
    print("=" * 80)

# ç”ŸæˆæŠ¥å‘Š
generate_comprehensive_report()

print("\n" + "=" * 60)
print("ğŸ‰ LASSOç®—æ³•å®éªŒå®Œæˆï¼")
print("ğŸ“Š ç»“æœæ–‡ä»¶:")
print("   - convergence_adaptive.png: æ”¶æ•›æ›²çº¿å¯¹æ¯”å›¾")
print("   - runtime_adaptive.png: è¿è¡Œæ—¶é—´å¯¹æ¯”å›¾")
print("   - comprehensive_analysis_adaptive.csv: è¯¦ç»†åˆ†ææ•°æ®")
print("=" * 60)